{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyP4ZySwF3y6QTEB1AkU8Fnm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "upGRW18zxELE",
        "outputId": "8ddbcf5b-db89-494d-f9bc-ec2253db95a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, TweetTokenizer\n",
        "from nltk.tokenize import MWETokenizer\n",
        "from nltk.tokenize.treebank import TreebankWordTokenizer\n",
        "from textblob import TextBlob\n",
        "import spacy\n",
        "import gensim\n",
        "from keras.preprocessing.text import text_to_word_sequence"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Text to tokenize\n",
        "text = \"Student life can be both exhilarating and stressful at the same time. 📚🎓 Managing assignments, exams, and social obligations can sometimes feel overwhelming, but it's also a time for growth and self-discovery. 🌱🌟 Amidst the hustle and bustle of campus life, it's crucial to find a balance between academic pursuits and personal well-being. Remembering to take breaks, prioritize self-care, and seek support when needed can make all the difference in navigating the challenges of university life. 🤝💆‍♂️ From late-night study sessions to spontaneous adventures with friends, each moment shapes the journey of the student experience. 🌌🚀\""
      ],
      "metadata": {
        "id": "dxipx0eAxOZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Word Tokenization\n",
        "nltk_tokens = nltk.word_tokenize(text)\n",
        "print (nltk_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ulq-sMJVyWeR",
        "outputId": "f10f8d71-dc49-445c-edd1-412f8948f228"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Student', 'life', 'can', 'be', 'both', 'exhilarating', 'and', 'stressful', 'at', 'the', 'same', 'time', '.', '📚🎓', 'Managing', 'assignments', ',', 'exams', ',', 'and', 'social', 'obligations', 'can', 'sometimes', 'feel', 'overwhelming', ',', 'but', 'it', \"'s\", 'also', 'a', 'time', 'for', 'growth', 'and', 'self-discovery', '.', '🌱🌟', 'Amidst', 'the', 'hustle', 'and', 'bustle', 'of', 'campus', 'life', ',', 'it', \"'s\", 'crucial', 'to', 'find', 'a', 'balance', 'between', 'academic', 'pursuits', 'and', 'personal', 'well-being', '.', 'Remembering', 'to', 'take', 'breaks', ',', 'prioritize', 'self-care', ',', 'and', 'seek', 'support', 'when', 'needed', 'can', 'make', 'all', 'the', 'difference', 'in', 'navigating', 'the', 'challenges', 'of', 'university', 'life', '.', '🤝💆\\u200d♂️', 'From', 'late-night', 'study', 'sessions', 'to', 'spontaneous', 'adventures', 'with', 'friends', ',', 'each', 'moment', 'shapes', 'the', 'journey', 'of', 'the', 'student', 'experience', '.', '🌌🚀']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word tokenization is the process of splitting a large sample of text into words. This is a requirement in natural language processing tasks where each word needs to be captured and subjected to further analysis like classifying and counting them for a particular sentiment etc. The Natural Language Tool kit(NLTK) is a library used to achieve this."
      ],
      "metadata": {
        "id": "yZ5A_S-Vq2nL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sentence Tokenization\n",
        "sentence_tokens = sent_tokenize(text)\n",
        "print(\"\\nSentence Tokenization:\", sentence_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q5bc0xJoybmg",
        "outputId": "706b9a75-b5d3-4fe6-88ad-3fd8f9ff5df4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence Tokenization: ['Student life can be both exhilarating and stressful at the same time.', \"📚🎓 Managing assignments, exams, and social obligations can sometimes feel overwhelming, but it's also a time for growth and self-discovery.\", \"🌱🌟 Amidst the hustle and bustle of campus life, it's crucial to find a balance between academic pursuits and personal well-being.\", 'Remembering to take breaks, prioritize self-care, and seek support when needed can make all the difference in navigating the challenges of university life.', '🤝💆\\u200d♂️ From late-night study sessions to spontaneous adventures with friends, each moment shapes the journey of the student experience.', '🌌🚀']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If the text is divided into several sentences inside the punctuation according to the paragraph provided, then it is Sentence Tokenization."
      ],
      "metadata": {
        "id": "XkNH1TqCsdYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Punctuation-based Tokenizer\n",
        "punct_tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
        "punct_tokens = punct_tokenizer.tokenize(text)\n",
        "print(\"\\nPunctuation-based Tokenizer:\", punct_tokens)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ejeasAfOzDhI",
        "outputId": "56bb8d79-00b8-4e1d-f58f-6e53d5ba4db5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Punctuation-based Tokenizer: ['Student', 'life', 'can', 'be', 'both', 'exhilarating', 'and', 'stressful', 'at', 'the', 'same', 'time', '.', '📚🎓', 'Managing', 'assignments', ',', 'exams', ',', 'and', 'social', 'obligations', 'can', 'sometimes', 'feel', 'overwhelming', ',', 'but', 'it', \"'\", 's', 'also', 'a', 'time', 'for', 'growth', 'and', 'self', '-', 'discovery', '.', '🌱🌟', 'Amidst', 'the', 'hustle', 'and', 'bustle', 'of', 'campus', 'life', ',', 'it', \"'\", 's', 'crucial', 'to', 'find', 'a', 'balance', 'between', 'academic', 'pursuits', 'and', 'personal', 'well', '-', 'being', '.', 'Remembering', 'to', 'take', 'breaks', ',', 'prioritize', 'self', '-', 'care', ',', 'and', 'seek', 'support', 'when', 'needed', 'can', 'make', 'all', 'the', 'difference', 'in', 'navigating', 'the', 'challenges', 'of', 'university', 'life', '.', '🤝💆\\u200d♂️', 'From', 'late', '-', 'night', 'study', 'sessions', 'to', 'spontaneous', 'adventures', 'with', 'friends', ',', 'each', 'moment', 'shapes', 'the', 'journey', 'of', 'the', 'student', 'experience', '.', '🌌🚀']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Punctuation Tokenizer is a process of splitting a paragraph or a sentence into words within a punctuation. That may include words, emojis, special characters and so on.."
      ],
      "metadata": {
        "id": "ragGbklStIiF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Treebank Word tokenizer\n",
        "treebank_tokenizer = TreebankWordTokenizer()\n",
        "treebank_tokens = treebank_tokenizer.tokenize(text)\n",
        "print(\"\\nTreebank Word tokenizer:\", treebank_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PBPRS5_-0o7h",
        "outputId": "d4244566-0eba-4ca3-a162-3c13b76477f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Treebank Word tokenizer: ['Student', 'life', 'can', 'be', 'both', 'exhilarating', 'and', 'stressful', 'at', 'the', 'same', 'time.', '📚🎓', 'Managing', 'assignments', ',', 'exams', ',', 'and', 'social', 'obligations', 'can', 'sometimes', 'feel', 'overwhelming', ',', 'but', 'it', \"'s\", 'also', 'a', 'time', 'for', 'growth', 'and', 'self-discovery.', '🌱🌟', 'Amidst', 'the', 'hustle', 'and', 'bustle', 'of', 'campus', 'life', ',', 'it', \"'s\", 'crucial', 'to', 'find', 'a', 'balance', 'between', 'academic', 'pursuits', 'and', 'personal', 'well-being.', 'Remembering', 'to', 'take', 'breaks', ',', 'prioritize', 'self-care', ',', 'and', 'seek', 'support', 'when', 'needed', 'can', 'make', 'all', 'the', 'difference', 'in', 'navigating', 'the', 'challenges', 'of', 'university', 'life.', '🤝💆\\u200d♂️', 'From', 'late-night', 'study', 'sessions', 'to', 'spontaneous', 'adventures', 'with', 'friends', ',', 'each', 'moment', 'shapes', 'the', 'journey', 'of', 'the', 'student', 'experience.', '🌌🚀']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The Treebank tokenizer is a type of tokenizer that it separates out clitics, that usually appear only in combination with another word, like \"I'm\",\"they'll\".\n",
        "It Treats most punctuation characters as separate tokens.\n",
        "It splits off comas and single quotes when they're followed by whitespace."
      ],
      "metadata": {
        "id": "4G7ErC6Kuy8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
        "\n",
        "# Detokenize using TreebankWordDetokenizer\n",
        "detokenizer = TreebankWordDetokenizer()\n",
        "detokenized_text = detokenizer.detokenize(treebank_tokens)\n",
        "\n",
        "print(\"Detokenized text:\", detokenized_text)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N-prn3wM1SQK",
        "outputId": "1f61c7bc-e4ac-4ef9-bef8-310f3c735e88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detokenized text: Student life can be both exhilarating and stressful at the same time. 📚🎓 Managing assignments, exams, and social obligations can sometimes feel overwhelming, but it's also a time for growth and self-discovery. 🌱🌟 Amidst the hustle and bustle of campus life, it's crucial to find a balance between academic pursuits and personal well-being. Remembering to take breaks, prioritize self-care, and seek support when needed can make all the difference in navigating the challenges of university life. 🤝💆‍♂️ From late-night study sessions to spontaneous adventures with friends, each moment shapes the journey of the student experience. 🌌🚀\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Just Experimented detokenizer for same and checked the result"
      ],
      "metadata": {
        "id": "frObpWmjvhwx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tweet Tokenizer\n",
        "tweet_tokenizer = TweetTokenizer()\n",
        "tweet_tokens = tweet_tokenizer.tokenize(text)\n",
        "print(\"\\nTweet Tokenizer:\", tweet_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mH5QzlU4uqP",
        "outputId": "3eb5f950-6460-4158-c589-9e93c2c2bda0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tweet Tokenizer: ['Student', 'life', 'can', 'be', 'both', 'exhilarating', 'and', 'stressful', 'at', 'the', 'same', 'time', '.', '📚', '🎓', 'Managing', 'assignments', ',', 'exams', ',', 'and', 'social', 'obligations', 'can', 'sometimes', 'feel', 'overwhelming', ',', 'but', \"it's\", 'also', 'a', 'time', 'for', 'growth', 'and', 'self-discovery', '.', '🌱', '🌟', 'Amidst', 'the', 'hustle', 'and', 'bustle', 'of', 'campus', 'life', ',', \"it's\", 'crucial', 'to', 'find', 'a', 'balance', 'between', 'academic', 'pursuits', 'and', 'personal', 'well-being', '.', 'Remembering', 'to', 'take', 'breaks', ',', 'prioritize', 'self-care', ',', 'and', 'seek', 'support', 'when', 'needed', 'can', 'make', 'all', 'the', 'difference', 'in', 'navigating', 'the', 'challenges', 'of', 'university', 'life', '.', '🤝', '💆\\u200d♂', '️', 'From', 'late-night', 'study', 'sessions', 'to', 'spontaneous', 'adventures', 'with', 'friends', ',', 'each', 'moment', 'shapes', 'the', 'journey', 'of', 'the', 'student', 'experience', '.', '🌌', '🚀']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is a specialized tokenizer designed to handle tweets and other social media text. Due to the distinctive features of tweets, such as hashtags, mentions, emojis, abbreviations, and URLs, they demand unique treatment during tokenization."
      ],
      "metadata": {
        "id": "WtZbtqUFnCLj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = '@aminbaybon HEllllllllo everybody i wish you a good #day'\n",
        "tokenizer = TweetTokenizer(preserve_case=False)\n",
        "tweet_tokens = tokenizer.tokenize(tweet)\n",
        "print(tweet_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YrRcxPpBla4v",
        "outputId": "be06bac2-0060-4678-b7cf-d795b119328a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['@aminbaybon', 'hellllllllo', 'everybody', 'i', 'wish', 'you', 'a', 'good', '#day']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = '@aminbaybon HEllllllllo everybody i wish you a good #day'\n",
        "tokenizer = TweetTokenizer(strip_handles=True)\n",
        "tweet_tokens = tokenizer.tokenize(tweet)\n",
        "print(tweet_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3YSdfzEXl8F0",
        "outputId": "3b609e38-a826-4aeb-a574-3ad3b3fc79db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['HEllllllllo', 'everybody', 'i', 'wish', 'you', 'a', 'good', '#day']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tweet = '@aminbaybon HEllllllllo everybody i wish you a good #day 415 123 1234'\n",
        "tokenizer = TweetTokenizer(preserve_case=False, reduce_len=False , strip_handles=True , match_phone_numbers = True)\n",
        "tweet_tokens = tokenizer.tokenize(tweet)\n",
        "print(tweet_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MkTbTIWcmjc_",
        "outputId": "26044b08-57e8-4665-9e1f-2d9d7c6aca55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['hellllllllo', 'everybody', 'i', 'wish', 'you', 'a', 'good', '#day', '415 123 1234']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enabling this parameter ensures that the phone number is tokenized as a single unit without any number divisions."
      ],
      "metadata": {
        "id": "ae-0G0wZm2cD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Multi-Word Expression Tokenizer\n",
        "mwe_tokenizer = MWETokenizer([('Student', 'life'), ('of', 'campus')])\n",
        "mwe_tokens = mwe_tokenizer.tokenize(word_tokenize(text))\n",
        "print(\"\\nMulti-Word Expression Tokenizer:\", mwe_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKSpgo2K4_TG",
        "outputId": "882c4673-08a4-4c1e-ff91-eceafc66684d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Multi-Word Expression Tokenizer: ['Student_life', 'can', 'be', 'both', 'exhilarating', 'and', 'stressful', 'at', 'the', 'same', 'time', '.', '📚🎓', 'Managing', 'assignments', ',', 'exams', ',', 'and', 'social', 'obligations', 'can', 'sometimes', 'feel', 'overwhelming', ',', 'but', 'it', \"'s\", 'also', 'a', 'time', 'for', 'growth', 'and', 'self-discovery', '.', '🌱🌟', 'Amidst', 'the', 'hustle', 'and', 'bustle', 'of_campus', 'life', ',', 'it', \"'s\", 'crucial', 'to', 'find', 'a', 'balance', 'between', 'academic', 'pursuits', 'and', 'personal', 'well-being', '.', 'Remembering', 'to', 'take', 'breaks', ',', 'prioritize', 'self-care', ',', 'and', 'seek', 'support', 'when', 'needed', 'can', 'make', 'all', 'the', 'difference', 'in', 'navigating', 'the', 'challenges', 'of', 'university', 'life', '.', '🤝💆\\u200d♂️', 'From', 'late-night', 'study', 'sessions', 'to', 'spontaneous', 'adventures', 'with', 'friends', ',', 'each', 'moment', 'shapes', 'the', 'journey', 'of', 'the', 'student', 'experience', '.', '🌌🚀']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multi-word tokenization, also known as phrase tokenization or multi-word expression (MWE) tokenization, is the process of splitting text into meaningful multi-word units rather than individual words. This is particularly useful in natural language processing (NLP) tasks where understanding the meaning of phrases or idiomatic expressions is important."
      ],
      "metadata": {
        "id": "iWVad9Jzo-wi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Textblob Tokenizer\n",
        "text = (\"Natural language processing (NLP) is a field \" +\n",
        "       \"of computer science, artificial intelligence \" +\n",
        "       \"and computational linguistics concerned with \" +\n",
        "       \"the interactions between computers and human \" +\n",
        "       \"(natural) languages, and, in particular, \" +\n",
        "       \"concerned with programming computers to \" +\n",
        "       \"fruitfully process large natural language \" +\n",
        "       \"corpora. Challenges in natural language \" +\n",
        "       \"processing frequently involve natural \" +\n",
        "       \"language understanding, natural language\" +\n",
        "       \"generation frequently from formal, machine\" +\n",
        "       \"-readable logical forms), connecting language \" +\n",
        "       \"and machine perception, managing human-\" +\n",
        "       \"computer dialog systems, or some combination \" +\n",
        "       \"thereof.\")\n",
        "\n",
        "# create a TextBlob object\n",
        "blob_object = TextBlob(text)\n",
        "\n",
        "# tokenize paragraph into words.\n",
        "print(\" Word Tokenize :\\n\", blob_object.words)\n",
        "\n",
        "# tokenize paragraph into sentences.\n",
        "print(\"\\n Sentence Tokenize :\\n\", blob_object.sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH7A0_2m5GRv",
        "outputId": "2054fd71-0262-466a-9535-a0f1fa6d52ad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Word Tokenize :\n",
            " ['Natural', 'language', 'processing', 'NLP', 'is', 'a', 'field', 'of', 'computer', 'science', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'natural', 'languages', 'and', 'in', 'particular', 'concerned', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', 'Challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', 'natural', 'languagegeneration', 'frequently', 'from', 'formal', 'machine-readable', 'logical', 'forms', 'connecting', 'language', 'and', 'machine', 'perception', 'managing', 'human-computer', 'dialog', 'systems', 'or', 'some', 'combination', 'thereof']\n",
            "\n",
            " Sentence Tokenize :\n",
            " [Sentence(\"Natural language processing (NLP) is a field of computer science, artificial intelligence and computational linguistics concerned with the interactions between computers and human (natural) languages, and, in particular, concerned with programming computers to fruitfully process large natural language corpora.\"), Sentence(\"Challenges in natural language processing frequently involve natural language understanding, natural languagegeneration frequently from formal, machine-readable logical forms), connecting language and machine perception, managing human-computer dialog systems, or some combination thereof.\")]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TextBlob is a Python library for processing textual data. Its tokenization functionality splits text into individual words or sentences."
      ],
      "metadata": {
        "id": "bD-9tEbqpc92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\")\n",
        "\n",
        "doc = nlp(\"Spacy is a library that comes under NLP (Natural Language Processing). It is an object-oriented Library that is used to deal with pre-processing of text, and sentences, and to extract information from the text using modules and functions.\")\n",
        "\n",
        "for token in doc:\n",
        "    print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z7dd4jLEyb7-",
        "outputId": "39ab938e-a326-48bf-c2fc-f4ad8a9b3e05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spacy\n",
            "is\n",
            "a\n",
            "library\n",
            "that\n",
            "comes\n",
            "under\n",
            "NLP\n",
            "(\n",
            "Natural\n",
            "Language\n",
            "Processing\n",
            ")\n",
            ".\n",
            "It\n",
            "is\n",
            "an\n",
            "object\n",
            "-\n",
            "oriented\n",
            "Library\n",
            "that\n",
            "is\n",
            "used\n",
            "to\n",
            "deal\n",
            "with\n",
            "pre\n",
            "-\n",
            "processing\n",
            "of\n",
            "text\n",
            ",\n",
            "and\n",
            "sentences\n",
            ",\n",
            "and\n",
            "to\n",
            "extract\n",
            "information\n",
            "from\n",
            "the\n",
            "text\n",
            "using\n",
            "modules\n",
            "and\n",
            "functions\n",
            ".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# spaCy Tokenizer\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "doc = nlp(text)\n",
        "spacy_tokens = [token.text for token in doc]\n",
        "print(\"\\nspaCy Tokenizer:\", spacy_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_uG_svI5u6-",
        "outputId": "a44a08f8-bdb2-4390-8804-93fdb0944276"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "spaCy Tokenizer: ['Natural', 'language', 'processing', '(', 'NLP', ')', 'is', 'a', 'field', 'of', 'computer', 'science', ',', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', '(', 'natural', ')', 'languages', ',', 'and', ',', 'in', 'particular', ',', 'concerned', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', '.', 'Challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', ',', 'natural', 'languagegeneration', 'frequently', 'from', 'formal', ',', 'machine', '-', 'readable', 'logical', 'forms', ')', ',', 'connecting', 'language', 'and', 'machine', 'perception', ',', 'managing', 'human', '-', 'computer', 'dialog', 'systems', ',', 'or', 'some', 'combination', 'thereof', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCy's tokenizer breaks raw text into tokens efficiently, handling languages, punctuation, contractions, and special characters intelligently, crucial for various NLP tasks."
      ],
      "metadata": {
        "id": "4rfPzLgyqIID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gensim word tokenizer\n",
        "gensim_tokens = gensim.utils.tokenize(text)\n",
        "print(\"\\nGensim word tokenizer:\", list(gensim_tokens))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "we7ZpVDh59fZ",
        "outputId": "67f689c4-c3d1-4a58-959a-983e469f5eaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Gensim word tokenizer: ['Natural', 'language', 'processing', 'NLP', 'is', 'a', 'field', 'of', 'computer', 'science', 'artificial', 'intelligence', 'and', 'computational', 'linguistics', 'concerned', 'with', 'the', 'interactions', 'between', 'computers', 'and', 'human', 'natural', 'languages', 'and', 'in', 'particular', 'concerned', 'with', 'programming', 'computers', 'to', 'fruitfully', 'process', 'large', 'natural', 'language', 'corpora', 'Challenges', 'in', 'natural', 'language', 'processing', 'frequently', 'involve', 'natural', 'language', 'understanding', 'natural', 'languagegeneration', 'frequently', 'from', 'formal', 'machine', 'readable', 'logical', 'forms', 'connecting', 'language', 'and', 'machine', 'perception', 'managing', 'human', 'computer', 'dialog', 'systems', 'or', 'some', 'combination', 'thereof']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Gensim, another popular NLP library, offers a word tokenizer that efficiently breaks text into tokens, providing support for various languages, punctuation, contractions, and special characters."
      ],
      "metadata": {
        "id": "FOQbkrDfqRnt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Keras, a deep learning library, provides tokenization tools that split text into tokens efficiently, handling languages, punctuation, contractions, and special characters."
      ],
      "metadata": {
        "id": "_s_2CQY1qdMu"
      }
    }
  ]
}